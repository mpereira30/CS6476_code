{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deep Learning](https://www.cc.gatech.edu/~hays/compvision/proj6/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import student_code as sc\n",
    "from torchvision.models import alexnet\n",
    "\n",
    "data_path = osp.join('../data', '15SceneData')\n",
    "num_classes = 15\n",
    "\n",
    "# If you have a good Nvidia GPU with an appropriate environment, \n",
    "# try setting the use_GPU flag to True (the environment provided does\n",
    "# not support GPUs and we will not provide any support for GPU\n",
    "# computation in this project). Please note that \n",
    "# we will evaluate your implementations only using CPU mode so even if\n",
    "# you use a GPU, make sure your code runs in the CPU mode with the\n",
    "# environment we provided. \n",
    "use_GPU = False\n",
    "if use_GPU:\n",
    "    from utils_gpu import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a network in PyTorch, we need 4 components:\n",
    "1. **Dataset** - an object which can load the data and labels given an index.\n",
    "2. **Model** - an object that contains the network architecture definition.\n",
    "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
    "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has two main parts. In Part 1, you will train a deep network from scratch. In Part 2, you will \"fine-tune\" a trained network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Warm up! Training a Deep Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to code anything for this part. You will simply run the code we provided, but we want you to report the result you got. This section will also familiarize you with the steps of training a deep network from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters.\n",
    "input_size = (64, 64)\n",
    "RGB = False  \n",
    "base_lr = 1e-2  # may try a smaller lr if not using batch norm\n",
    "weight_decay = 5e-4\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create our datasets, by calling the create_datasets function from student_code. This function returns a separate dataset loader for each split of the dataset (training and testing/validation). Each dataloader is used to load the datasets after appling some pre-processing transforms. In Part 1, you will be asked to add a few more pre-processing transforms to the dataloaders by modifying this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our network model using the SimpleNet class from student_code. The implementation provided in the SimpleNet class gives you a basic network. In Part 1, you will be asked to add a few more layers to this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the network model.\n",
    "model = sc.SimpleNet(num_classes=num_classes, rgb=False, verbose=False)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function.\n",
    "# see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer and a learning rate scheduler\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "# Currently a simple step scheduler.\n",
    "# See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "# and how to use them\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to train our network! We will start a local server to see the training progress of our network. Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 1 on the top left bar where is says Environment (only select Part 1, do not check main or Part 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network!\n",
    "params = {'n_epochs': 100, 'batch_size': 50, 'experiment': 'part1'}\n",
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect this code to take around 5 minutes on CPU or 3 minutes on GPU. Now you are ready to actually modify the functions we used to train our model. Before you move on, make sure to record the accuracy of your network from Part 0, and report it in your write up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Modifying the Dataloaders and the Simple Network create_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will modify the create_datasets function from student_code. You will add random left-right mirroring and normalization to the transformations applied to the training dataset. You will also add normalization to the transformations applied to the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will modify SimpleNet by adding droppout, batch normalization, and additional convolution/maxpool/relu layers. You should achieve an accuracy of at least **50%**. Make sure your network passes this threshold--it is required for full credit on this section!\n",
    "\n",
    "You can also use the following two blocks to determine the stucture of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the network model\n",
    "model = sc.SimpleNet(num_classes=num_classes, rgb=False, verbose=False)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this block to determine the kernel size of the conv2d layer in the classifier\n",
    "# first, set the kernel size of that conv2d layer to 1, and run this block\n",
    "# then, use that size of input to the classifier printed by this block to\n",
    "# go back and update the kernel size of the conv2d layer in the classifier\n",
    "# Finally, run this block again and verify that the network output size is a scalar\n",
    "# Don't forget to re-run the block above every time you update the SimpleNet class!\n",
    "from torch.autograd import Variable\n",
    "data, _ = train_dataset[0]\n",
    "s = data.size()\n",
    "data = Variable(data.view(1, *s))\n",
    "if use_GPU:\n",
    "    data = data.cuda()\n",
    "out = model(data)\n",
    "print('Network output size is ', out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. You do not have to modify the custom_part1_trainer in student_code if you use the same loss_function, optimizer, scheduler and parameters (n_epoch, batch_size etc.) as provided in this notebook to hit the required threshold of 50% accuracy. If you changed any of these values, it is important that you modify this function in student_code since we will not be using the notebook you submit to evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer. You can modify custom_part1_trainer in\n",
    "# student_copy.py if you want to try different learning settings.\n",
    "custom_part1_trainer = sc.custom_part1_trainer(model)\n",
    "\n",
    "if custom_part1_trainer is None:\n",
    "    # Create the loss function.\n",
    "    # see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create the optimizer and a learning rate scheduler.\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "    # Currently a simple step scheduler, but you can get creative.\n",
    "    # See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "    # and how to use them\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n",
    "\n",
    "    params = {'n_epochs': 100, 'batch_size': 50, 'experiment': 'part1'}\n",
    "    \n",
    "else:\n",
    "    if 'loss_function' in custom_part1_trainer:\n",
    "        loss_function = custom_part1_trainer['loss_function']\n",
    "    if 'optimizer' in custom_part1_trainer:\n",
    "        optimizer = custom_part1_trainer['optimizer']\n",
    "    if 'lr_scheduler' in custom_part1_trainer:\n",
    "        lr_scheduler = custom_part1_trainer['lr_scheduler']\n",
    "    if 'params' in custom_part1_trainer:\n",
    "        params = custom_part1_trainer['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train our network! As before, we will start a local server to see the training progress of our network (if you server is already running, you should not start another one). Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 1 on the top left bar where is says Environment (only select Part 1, do not check main or Part 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network!\n",
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you get at least 50% accuracy in this section! If you tried different settings than the ones provided to get 50%, you should modify custom_part1_trainer in student code to return a dictionary with your changed settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Fine-Tuning a Pre-Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a network from scratch takes a lof of time. Instead of training from scratch, we can take a pre-trained model and fine tune it for our purposes. This is the goal of Part 2--you will train a pre-trained network, and achieve at least 80% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "input_size = (224, 224)\n",
    "RGB = True\n",
    "base_lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "momentum = 0.9\n",
    "backprop_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block loads a pretrained AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network model.\n",
    "model = alexnet(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you modify create_part2_model from student code in order to fine-tune AlexNet. As you can see in the docs (https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py) and in the model printout above, AlexNet has 2 parts: 'features', which constists of conv layers that extract feature maps from the image, and 'classifier' which consists of FC layers that classify the features. We want to replace the last Linear layer in model.classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = sc.create_part2_model(model, num_classes)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. Just as with part 1, if you modify any of the setttings to hit the required accuracy, you must modify custom_part2_trainer function to return a dictionary containing your changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer. You can modify custom_part2_trainer in\n",
    "# student_copy.py if you want to try different learning settings.\n",
    "custom_part2_trainer = sc.custom_part2_trainer(model)\n",
    "\n",
    "if custom_part2_trainer is None:\n",
    "    # Create the loss function\n",
    "    # see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Since we do not want to optimize the whole network, we must extract a list of parameters of interest that will be\n",
    "    # optimized by the optimizer.\n",
    "    params_to_optimize = []\n",
    "\n",
    "    # List of modules in the network\n",
    "    mods = list(model.features.children()) + list(model.classifier.children())\n",
    "\n",
    "    # Extract parameters from the last `backprop_depth` modules in the network and collect them in\n",
    "    # the params_to_optimize list.\n",
    "    for m in mods[::-1][:backprop_depth]:\n",
    "        params_to_optimize.extend(list(m.parameters()))\n",
    "\n",
    "    # Construct the optimizer    \n",
    "    optimizer = optim.SGD(params=params_to_optimize, lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "    # Create a scheduler, currently a simple step scheduler, but you can get creative.\n",
    "    # See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "    # and how to use them\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    params = {'n_epochs': 4, 'batch_size': 10, 'experiment': 'part2'} \n",
    "    \n",
    "else:\n",
    "    if 'loss_function' in custom_part2_trainer:\n",
    "        loss_function = custom_part2_trainer['loss_function']\n",
    "    if 'optimizer' in custom_part2_trainer:\n",
    "        optimizer = custom_part2_trainer['optimizer']\n",
    "    if 'lr_scheduler' in custom_part2_trainer:\n",
    "        lr_scheduler = custom_part2_trainer['lr_scheduler']\n",
    "    if 'params' in custom_part2_trainer:\n",
    "        params = custom_part2_trainer['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to fine tune our network! Just like before, we will start a local server to see the training progress of our network. Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 2 on the top left bar where is says Environment (only select Part 2, do not check main or Part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network!\n",
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect this code to take around 10 minutes on CPU or 30 seconds on GPU. You should hit 80% accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
